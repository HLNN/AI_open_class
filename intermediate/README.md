# AI开放实验中期测试

-----------------

## 简答

 - 为什么Sigmoid和Tanh激活函数会导致梯度消失？有什么解决方案？ 
 - ReLu系列的激活函数的优点是什么？有什么局限性以及如何改进？
 - 写出多层感知机的平方误差和交叉熵损失函数
 - 平方误差损失函数和交叉熵损失函数分别适合什么场景？
 - 模型的方差与偏差的关系。
 - 如何解决模型的过拟合与欠拟合？
 - 卷积神经网络相较于MLP的优点？


**为什么Sigmoid和Tanh激活函数会导致梯度消失？有什么解决方案？**

Sigmoid和Tanh都是有限激活函数，在靠近饱和区的时候导数小，容易出现梯度消失的问题。


**ReLu系列的激活函数的优点是什么？有什么局限性以及如何改进？**

优点：

 - 计算复杂度低，不用进行指数运算
 - 便于反向传播
 - 部分神经元输出为0，提高网络稀疏性，减少参数的相互依存关系，缓解过拟合问题

缺点：

 - 不是零均值(zero-centered)输出
 - 部分神经元可能出现坏死的情况，永远不会被激活
 - 不会对输出信号的幅值进行压缩
 
改进：

 - Leakly ReLU
 
f(x) = max(0.01x, x) 解决神经元坏死的问题

 - ELU函数
 
![](https://latex.codecogs.com/gif.latex?f\left&space;(&space;x&space;\right&space;)=\begin{cases}&space;x&space;&&space;\text{&space;if&space;}&space;x%3E0&space;\\\\&space;\alpha&space;\left&space;(&space;e^{x}-1&space;\right&space;)&space;&&space;otherwose&space;\end{cases})

解决神经元坏死的问题且输出均值接近零，但计算量大


**写出多层感知机的平方误差和交叉熵损失函数**

 - 平方误差：各点模型输出值和理论值之差平方的平均值
 
 - 交叉熵：
 ![](https://img-blog.csdn.net/20160402172100739)


**平方误差损失函数和交叉熵损失函数分别适合什么场景？**

平方损失适合输出为连续，最后一层不含sigmoid或者 softmax的激活函数的神经网络
交叉熵损失更适合二分类或者多分类的场景


**模型的方差与偏差的关系。**

 - 方差：随机变量的离散程度，体现随机变量的波动
 - 偏差：预测相对于实际的误差大小


**如何解决模型的过拟合与欠拟合？**

 过拟合：
  - 清洗数据或加大数据量
  - 减少正则化参数
  - 采用dropout
  
欠拟合：
 - 添加特征
 - 增加正则化参数


**卷积神经网络相较于MLP的优点？**

 - 提取体现网络对局部的感知，能提取出图片的特征
 - 共享参数，减小网络和参数的复杂度


